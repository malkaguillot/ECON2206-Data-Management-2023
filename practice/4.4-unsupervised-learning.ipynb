{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data management\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "## [Michel Coppée](https://www.uliege.be/cms/c_9054334/fr/repertoire?uid=u224042) & [Malka Guillot](https://malkaguillot.github.io/)\n",
    "\n",
    "## HEC Liège | [ECON2306]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns; \n",
    "sns.set_theme()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "In scikit-learn, PCA is implemented as a `transformer` object that learns $n$ components in its `fit` method, and can be used on new data to project it on these components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate some data\n",
    "X = np.random.normal(0, 1, (100, 4))\n",
    "X[:,2] = 3 * X[:,0] - 2 * X[:,1] + np.random.normal(0, 0.1, 100)\n",
    "X[:,3] = 1.5 * X[:,0] - 0.5 * X[:,1] + np.random.normal(0, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each feature will have zero mean\n",
    "X = X - np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if j > i:\n",
    "            plt.subplot(4,4,i*4+j+1)\n",
    "            plt.scatter(X[:,i], X[:,j])\n",
    "            plt.xlabel(f'x{i+1}', fontsize=20)\n",
    "            plt.ylabel(f'x{j+1}', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- x1 and x2 do not seem correlated\n",
    "- x1 seems very correlated with both x3 and x4\n",
    "- x2 seems somewhat correlated with both x3 and x4\n",
    "- x3 and x4 seem very correlated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA minimal working example\n",
    "\n",
    "Note: `PCA` centers the input data automatically. To scale the variables so that they have a unit variance, you can use the optional parameter `whiten=True`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get PCA Components\n",
    "\n",
    "We use the `PCA` module from `sllearn` to form the Principal Components\n",
    "\n",
    "Then, we can project the data into the new feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize; reduce to 3 features with PCA (from 4)\n",
    "pca = PCA(n_components=3, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit and transform data\n",
    "pca_features = pca.fit_transform(X)\n",
    "pca_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca_features) # for each observation, we have the coordinates in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "pca_df = pd.DataFrame(\n",
    "    data=pca_features, \n",
    "    columns=['PC1', 'PC2', 'PC3'])\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 7))\n",
    "\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "scatter_plot= ax.scatter(X[:,1], X[:,0], X[:,2], c=X[:,3], cmap='Greens');\n",
    "plt.colorbar(scatter_plot)\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X0')\n",
    "ax.set_zlabel('X2')\n",
    "ax.zaxis.labelpad=-0.8\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA model attribute plots\n",
    "\n",
    "PCA components and their significance can be explained using following attributes\n",
    "\n",
    "- **Explained variance** is the amount of variance explained by each of the selected components. This attribute is associated with the sklearn PCA model as `explained_variance_`\n",
    "\n",
    "- **Explained variance ratio** is the percentage of variance explained by each of the selected components. It’s attribute is `explained_variance_ratio_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `explained_variance_` plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1,len(pca.explained_variance_ )+1),pca.explained_variance_ )\n",
    "plt.ylabel('Explained variance')\n",
    "plt.xlabel('Components')\n",
    "plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "         np.cumsum(pca.explained_variance_),\n",
    "         c='red',\n",
    "         label=\"Cumulative Explained Variance\")\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `explained_variance_ratio_` plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot([1,2,3], pca.explained_variance_ratio_, '-o', label='Individual component')\n",
    "plt.plot([1,2,3], np.cumsum(pca.explained_variance_ratio_), '-s', label='Cumulative')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.xlim(0.75,4.25)\n",
    "plt.ylim(0,1.05)\n",
    "plt.xticks([1,2,3])\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D Scatter plot of PC1 and PC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_df['PC1'], pca_df['PC2'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D Scatter plot of PC1,PC2 and PC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 7))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "scatter_plot= ax.scatter(pca_df['PC1'], pca_df['PC2'],pca_df['PC3']);\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.zaxis.labelpad=-0.8\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Plotly figure\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = px.scatter_3d(x=pca_df['PC1'],\n",
    "                    y=pca_df['PC2'],\n",
    "                    z=pca_df['PC3'], color=pca_df['PC3'], opacity=0.7,\n",
    "                    )\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "                    xaxis_title='PC1',\n",
    "                    yaxis_title='PC2',\n",
    "                    zaxis_title='PC3'),\n",
    "                    width=700,\n",
    "margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal components correlation coefficients\n",
    "loadings = pca.components_\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features before PCA\n",
    "n_features = pca.n_features_in_\n",
    " \n",
    "# Feature names before PCA\n",
    "feature_names = [0, 1, 2, 3]\n",
    " \n",
    "# PC names\n",
    "pc_list = [f'PC{i}' for i in list(range(1, n_features + 1))]\n",
    " \n",
    "# Match PC names to loadings\n",
    "pc_loadings = dict(zip(pc_list, loadings))\n",
    " \n",
    "# Matrix of corr coefs between feature names and PCs\n",
    "loadings_df = pd.DataFrame.from_dict(pc_loadings)\n",
    "loadings_df['feature_names'] = feature_names\n",
    "loadings_df = loadings_df.set_index('feature_names')\n",
    "loadings_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the correlation coefficients (loadings) of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loadings of x and y axes\n",
    "xs = loadings[0]\n",
    "ys = loadings[1]\n",
    "zs = loadings[2]\n",
    " \n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "scatter_plot= ax.scatter(pca_df['PC1'], pca_df['PC2'],pca_df['PC3'], \n",
    "    c=pca_df['PC3'],\n",
    "    cmap='Greens', \n",
    "    alpha=0.5)\n",
    "\n",
    "fig = px.scatter_3d(x=pca_df['PC1'],\n",
    "                    y=pca_df['PC2'],\n",
    "                    z=pca_df['PC3'], color=pca_df['PC3'], opacity=0.7,\n",
    "                    )\n",
    "\n",
    "# Plot the loadings on a scatterplot\n",
    "for i, varnames in enumerate(feature_names):\n",
    "    ax.scatter(xs[i], ys[i], zs[i], s=100)\n",
    "    ax.text(xs[i], ys[i],zs[i], varnames)\n",
    "    ax.quiver(0, 0, 0, xs[i], ys[i],zs[i], color='red',arrow_length_ratio=0.1)  \n",
    "\n",
    "ax.set_xlabel('PC1', rotation=-10)\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Show plot\n",
    "plt.title('3D Loading plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `USAarrests` data\n",
    "\n",
    "\n",
    "For each of the 50 states in the United States, the data set contains \n",
    "- the number of arrests per $100, 000$ residents \n",
    "- for each of three crimes: `Assault` ,` Murder` ,and `Rape` . \n",
    "- Other variable: `UrbanPop` (the % of the population in each state living in urban areas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parent_path=os.path.dirname(os.getcwd()) # os.getcwd() fetchs the current path, \n",
    "data_path=os.path.join(parent_path, 'data')\n",
    "df = pd.read_csv(os.path.join(data_path, 'USArrests.csv'), index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pre-processing\n",
    "PCA should be performed after standardizing each variable to have mean zero and standard deviation one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X = pd.DataFrame(scale(df), index=df.index, columns=df.columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3D scattered points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn: Plot the 3D scatterplot of the data, with color intensity for the fourth variable </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn:  </h3>\n",
    "\n",
    "- Initialize a PCA with 4 components \n",
    "- `Fit` the PCA model and `transform` X to get the principal components \n",
    "- Compute the loading vectors \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn: Plot the 3D scatterplot of the pca features </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn: plot the proportion of Variance Explained by principal component </h3>\n",
    "\n",
    "How many components do you chose? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering Methods\n",
    "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generate a two-dimensional dataset containing four distinct blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets.samples_generator import make_blobs # random datapoints in 2D\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "#y_kmeans # Each instance was assigned to one of the 4 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_ # The 4 centroïds estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prediction of the labels of new instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3>\n",
    " Your turn: fit a `KMeans` algorithm on the same data with $6$ clusters and plot the result in 2D \n",
    " </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Finding the optimal number of clusers: Inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42, n_init=10).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow',\n",
    "             xy=(4, inertias[3]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Finding the optimal number of clusers: Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.axis([1.8, 8.5, 0.3, 0.7])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agglo = AgglomerativeClustering(n_clusters=5)\n",
    "agglo.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "labels = agglo.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "agglo2 = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "agglo2.fit(X)\n",
    "labels = agglo2.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrogram plot (with `scipy`)\n",
    "\n",
    "We can use Scipy's `hierarchy.linkage()` to form clusters and plot them with `hierarchy.dendrogram()`:\n",
    "The three big dendrograms correspond to the clusters with largest distances among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "hierarchy.dendrogram(hierarchy.linkage(X, 'single'),\n",
    "            orientation='top',\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "\n",
    "# Plotting a horizontal line based on the first biggest distance between clusters \n",
    "plt.axhline(0.9, color='red', linestyle='--'); \n",
    "# Plotting a horizontal line based on the second biggest distance between clusters \n",
    "plt.axhline(.62, color='crimson'); \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how the Dendrogram is only a reference when used to choose the number of clusters. We already know that we have 4 clusters in the dataset, but if we were to determine their number by the Dendrogram, 3 would be our first option, and 4 would be our second option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
